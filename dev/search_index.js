var documenterSearchIndex = {"docs":
[{"location":"examples/ex_basics/#Basic-Example","page":"TBD","title":"Basic Example","text":"","category":"section"},{"location":"examples/ex_basics/","page":"TBD","title":"TBD","text":"...","category":"page"},{"location":"refs/api/#API","page":"API","title":"API","text":"","category":"section"},{"location":"refs/api/#Index","page":"API","title":"Index","text":"","category":"section"},{"location":"refs/api/","page":"API","title":"API","text":"Pages   = [\"api.md\"]\nOrder   = [:function, :type]","category":"page"},{"location":"refs/api/#References","page":"API","title":"References","text":"","category":"section"},{"location":"refs/api/","page":"API","title":"API","text":"Modules = [MPITape]\nPages   = [\"api.jl\", \"printing.jl\", \"fileio.jl\", \"plotting.jl\", \"communication_graph.jl\"]","category":"page"},{"location":"refs/api/#MPITape.MPIEventTrace","page":"API","title":"MPITape.MPIEventTrace","text":"Holding data that is required to be exchanged between prehook and posthook  (i.e. the start time of the call)\n\n\n\n\n\n","category":"type"},{"location":"refs/api/#MPITape.print_merged-Tuple{Any}","page":"API","title":"MPITape.print_merged","text":"print_merged(tape; color)\n\n\nPrints the given tape with the assumption that it contains events from multiple MPI ranks. Typically, the input should be the result of MPITape.readll_and_merge() or similar.\n\n\n\n\n\n","category":"method"},{"location":"refs/api/#MPITape.print_mytape-Tuple{}","page":"API","title":"MPITape.print_mytape","text":"print_mytape(; showrank)\n\n\nPrint the local tape (of the calling MPI rank).\n\n\n\n\n\n","category":"method"},{"location":"refs/api/#MPITape.readall_and_merge","page":"API","title":"MPITape.readall_and_merge","text":"readall_and_merge()\nreadall_and_merge(dir; prefix)\n\n\nReads all tapes from disk and merges them into a single \"merged tape\".\n\n\n\n\n\n","category":"function"},{"location":"refs/api/#MPITape.plot_merged-Tuple{Array{MPITape.MPIEvent}}","page":"API","title":"MPITape.plot_merged","text":"plot_merged(tape; palette, fname)\n\n\nPlots a gantt chart of the recorded MPI API calls and store it to a file. Additionally draws arrows between communicating MPIEvents.\n\n\n\n\n\n","category":"method"},{"location":"refs/api/#MPITape.plot_sequence_merged-Tuple{Any}","page":"API","title":"MPITape.plot_sequence_merged","text":"plot_sequence_merged(tape)\n\n\nPlot a sequence diagram of the communication between ranks using the communication graph created by get_edges.\n\nCreates a plot of the following form using Kroki.jl:\n\n┌──────┐          ┌──────┐          ┌──────┐          ┌──────┐          ┌──────┐ │Rank0│          │Rank1│          │Rank2│          │Rank3│          │Rank4│ └──┬───┘          └──┬───┘          └──┬───┘          └──┬───┘          └──┬───┘    │     MPISend    │                 │                 │                 │        │ ────────────────>                 │                 │                 │        │                 │                 │                 │                 │          │              MPISend             │                 │                 │        │ ──────────────────────────────────>                 │                 │        │                 │                 │                 │                 │        │                 │     MPISend    │                 │                 │        │ ────────────────────────────────────────────────────>                 │                    │                 │                 │                 │                 │        │                 │              MPISend             │                 │        │ ──────────────────────────────────────────────────────────────────────>           │                 │                 │                 │                 │        │     MPISend    │                 │                 │                 │        │ <────────────────                 │                 │                 │        │                 │                 │                 │                 │        │              MPISend             │                 │                 │        │ <──────────────────────────────────                 │                 │        │                 │                 │                 │                 │        │                 │     MPISend    │                 │                 │        │ <────────────────────────────────────────────────────                 │        │                 │                 │                 │                 │        │                 │              MPISend             │                 │        │ <──────────────────────────────────────────────────────────────────────     ┌──┴───┐          ┌──┴───┐          ┌──┴───┐          ┌──┴───┐          ┌──┴───┐ │Rank0│          │Rank1│          │Rank2│          │Rank3│          │Rank4│ └──────┘          └──────┘          └──────┘          └──────┘          └──────┘\n\n\n\n\n\n","category":"method"},{"location":"refs/api/#MPITape.MPIEventNeighbors-Tuple{MPITape.MPIEvent}","page":"API","title":"MPITape.MPIEventNeighbors","text":"MPIEventNeighbors(ev)\n\n\nCreates a MPIEventNeighbors from an MPIEvent. This struct is used to keep trakc of the communication pairs for each event while constructing the edges of the communication graph.\n\n\n\n\n\n","category":"method"},{"location":"refs/api/#MPITape.get_edges-Tuple{Array{MPITape.MPIEvent}}","page":"API","title":"MPITape.get_edges","text":"get_edges(tape; check)\n\n\nGenerates the edges of a directed communication graph, where the edges represent  communication between two MPIEvents.\n\nThe method returns an Array of Tuple{MPIEvent, MPIEvent}. Every tuple directed edge between MPI calls that exchanged data. In consequence, a MPISend call and its matching MPIRecv call will result in a single edge in the graph whereas a MPI_Bcast over n ranks will lead to n - 1 edges since the root will exchange data with all other ranks.\n\nThe methods checks for completeness of the created graph and throws an error, if not all MPI calls can be matched. Setting check = false will skip these tests.\n\n\n\n\n\n","category":"method"},{"location":"#MPITape.jl","page":"MPITape","title":"MPITape.jl","text":"","category":"section"},{"location":"#Installation","page":"MPITape","title":"Installation","text":"","category":"section"},{"location":"","page":"MPITape","title":"MPITape","text":"Note: Only Linux is supported!","category":"page"},{"location":"","page":"MPITape","title":"MPITape","text":"The package is not registered. You can use","category":"page"},{"location":"","page":"MPITape","title":"MPITape","text":"] add https://github.com/pc2/MPITape.jl","category":"page"},{"location":"","page":"MPITape","title":"MPITape","text":"to add the package to your Julia environment.","category":"page"},{"location":"#Prerequisites","page":"MPITape","title":"Prerequisites","text":"","category":"section"},{"location":"","page":"MPITape","title":"MPITape","text":"TBD","category":"page"},{"location":"#Noteworthy-Alternatives","page":"MPITape","title":"Noteworthy Alternatives","text":"","category":"section"},{"location":"","page":"MPITape","title":"MPITape","text":"TBD","category":"page"},{"location":"#Acknowledgements","page":"MPITape","title":"Acknowledgements","text":"","category":"section"},{"location":"","page":"MPITape","title":"MPITape","text":"CI infrastructure is provided by the Paderborn Center for Parallel Computing (PC²)","category":"page"}]
}
